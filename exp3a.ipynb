{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_EXP3a_PATH = \"/Users/liangxinyu/Desktop/XinyuLiang-prompting/datasets/exp3/blimp/corpus.csv\"\n",
    "NUM_SAMPLES = 390 \n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATES_EXP3a = {\n",
    "    \"MetaQuestionSimple\": (\n",
    "        \"Is the following sentence a good sentence of English?\\n{sentence}\\n\"\n",
    "    ),\n",
    "    \"MetaInstruct\": (\n",
    "        \"You are a helpful writing assistant.\\n\"\n",
    "        \"Is the following sentence a good sentence of English?\\n{sentence}\\n\"\n",
    "    ),\n",
    "    \"MetaQuestionComplex\": (\n",
    "        \"Here is a sentence:\\n{sentence}\\n\"\n",
    "        \"Is this a good sentence of English?\\n\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name):\n",
    "    \"\"\"Loads the model and tokenizer from Hugging Face.\"\"\"\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    print(f\"Loading model: {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "    ).to(DEVICE)\n",
    "    model.eval()\n",
    "    print(\"Model loaded.\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_dataset_exp3a(filepath, num_samples, seed):\n",
    "\n",
    "    print(f\"Loading dataset for Experiment 3a from: {filepath}\")\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Dataset file not found at: {filepath}. Please update the DATASET_EXP3a_PATH variable.\")\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    required_cols = {'good_sentence', 'bad_sentence'}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(f\"Dataset must contain the columns: {list(required_cols)}\")\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        \"good_sentence\": \"grammatical\",\n",
    "        \"bad_sentence\": \"ungrammatical\"\n",
    "    })\n",
    "    \n",
    "    print(f\"Loaded and renamed {len(df)} total items.\")\n",
    "    print(f\"Subsampling to {num_samples} items...\")\n",
    "    \n",
    "    dataset_sample_df = df.sample(n=num_samples, random_state=seed)\n",
    "    \n",
    "    return dataset_sample_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_sentence_log_prob(model, tokenizer, sentence):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\").to(DEVICE)\n",
    "        input_ids = inputs.input_ids\n",
    "\n",
    "        inputs.pop(\"token_type_ids\", None)\n",
    "        \n",
    "        # For very short sentences, handle edge case\n",
    "        if input_ids.shape[1] <= 1:\n",
    "            return 0.0\n",
    "\n",
    "        outputs = model(**inputs, labels=input_ids)\n",
    "        \n",
    "        log_likelihood = -outputs.loss.item() * (input_ids.shape[1] - 1)\n",
    "        \n",
    "        return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs_of_options(model, tokenizer, text_prompt, options):\n",
    "    \"\"\"\n",
    "    'Yes' / 'No' as the very next token:\n",
    "    Encode [prompt] and [prompt + option] and take the first token\n",
    "    after the prompt boundary for each option.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # next token is the answer\n",
    "        prompt = text_prompt.rstrip() + \"\\n\"\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        inputs.pop(\"token_type_ids\", None)\n",
    "        logits_next = model(**inputs).logits[0, -1, :]\n",
    "        log_probs = torch.nn.functional.log_softmax(logits_next, dim=-1)\n",
    "\n",
    "        prompt_ids = tokenizer(prompt, add_special_tokens=False).input_ids\n",
    "        scores = []\n",
    "        for opt in options:\n",
    "            opt_clean = opt.strip()\n",
    "            concat_ids = tokenizer(prompt + opt_clean, add_special_tokens=False).input_ids\n",
    "            if len(concat_ids) <= len(prompt_ids):\n",
    "                concat_ids = tokenizer(prompt + \" \" + opt_clean, add_special_tokens=False).input_ids\n",
    "            if len(concat_ids) <= len(prompt_ids):\n",
    "                raise ValueError(\"Could not identify answer boundary for option: {}\".format(opt_clean))\n",
    "            first_token_id = concat_ids[len(prompt_ids)]\n",
    "            scores.append(log_probs[first_token_id].item())\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_3a(model, tokenizer, dataset):\n",
    "\n",
    "    results = []\n",
    "    print(f\"\\nRunning Experiment 3a on {len(dataset)} items...\")\n",
    "    \n",
    "    for item in tqdm(dataset):\n",
    "        grammatical_sent = item[\"grammatical\"]\n",
    "        ungrammatical_sent = item[\"ungrammatical\"]\n",
    "        \n",
    "        item_results = {\"grammatical\": grammatical_sent, \"ungrammatical\": ungrammatical_sent}\n",
    "\n",
    "        # Direct\n",
    "        item_results['direct_log_prob_grammatical'] = get_full_sentence_log_prob(model, tokenizer, grammatical_sent)\n",
    "        item_results['direct_log_prob_ungrammatical'] = get_full_sentence_log_prob(model, tokenizer, ungrammatical_sent)\n",
    "\n",
    "        # Metalinguistic\n",
    "        options = [\"Yes\", \"No\"]\n",
    "        for method_name, template in PROMPT_TEMPLATES_EXP3a.items():\n",
    "            # GRAMMATICAL\n",
    "            prompt_grammatical = template.format(sentence=grammatical_sent)\n",
    "            log_probs_grammatical = get_log_probs_of_options(model, tokenizer, prompt_grammatical, options)\n",
    "            item_results[f'{method_name}_log_prob_yes_grammatical'] = log_probs_grammatical[0]\n",
    "            item_results[f'{method_name}_log_prob_no_grammatical'] = log_probs_grammatical[1]\n",
    "\n",
    "            # UNGRAMMATICAL\n",
    "            prompt_ungrammatical = template.format(sentence=ungrammatical_sent)\n",
    "            log_probs_ungrammatical = get_log_probs_of_options(model, tokenizer, prompt_ungrammatical, options)\n",
    "            item_results[f'{method_name}_log_prob_yes_ungrammatical'] = log_probs_ungrammatical[0]\n",
    "            item_results[f'{method_name}_log_prob_no_ungrammatical'] = log_probs_ungrammatical[1]\n",
    "\n",
    "        results.append(item_results)\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##        ANALYSIS & PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results_exp3a(df):\n",
    "\n",
    "    print(\"\\n\" + \"=\"*20 + \" Experiment 3a: Full Reproduction Results \" + \"=\"*20)\n",
    "    print(f\"Model used: {MODEL_NAME}\")\n",
    "\n",
    "    print(\"\\n--- Task Performance (Accuracy) ---\")\n",
    "    print(\"Direct method uses standard accuracy. Metalinguistic methods use Balanced Accuracy.\\n\")\n",
    "    \n",
    "    # Direct\n",
    "    direct_correct = (df['direct_log_prob_grammatical'] > df['direct_log_prob_ungrammatical']).sum()\n",
    "    direct_accuracy = direct_correct / len(df)\n",
    "    print(f\"{'direct':<22}: {direct_accuracy:.2%}\")\n",
    "\n",
    "    # Metalinguistic\n",
    "    for method in PROMPT_TEMPLATES_EXP3a.keys():\n",
    "        # True Positives: Grammatical sentences with P(Yes) > P(No)\n",
    "        tp = (df[f'{method}_log_prob_yes_grammatical'] > df[f'{method}_log_prob_no_grammatical']).sum()\n",
    "        # True Negatives: Ungrammatical sentences with P(No) > P(Yes)\n",
    "        tn = (df[f'{method}_log_prob_no_ungrammatical'] > df[f'{method}_log_prob_yes_ungrammatical']).sum()\n",
    "        \n",
    "        tpr = tp / len(df)  # TP\n",
    "        tnr = tn / len(df)  # TN\n",
    "        \n",
    "        balanced_accuracy = (tpr + tnr) / 2\n",
    "        print(f\"{method:<22}: {balanced_accuracy:.2%}\")\n",
    "\n",
    "    print(\"\\n--- Internal Consistency (Pearson Correlation) ---\")\n",
    "    print(\"Correlation of log-prob differentials between Direct and Metalinguistic methods.\\n\")\n",
    "    \n",
    "    # Direct differential\n",
    "    df['direct_diff'] = df['direct_log_prob_grammatical'] - df['direct_log_prob_ungrammatical']\n",
    "\n",
    "    # Metalinguistic differential\n",
    "    for method in PROMPT_TEMPLATES_EXP3a.keys():\n",
    "        df[f'{method}_diff'] = df[f'{method}_log_prob_yes_grammatical'] - df[f'{method}_log_prob_yes_ungrammatical']\n",
    "        \n",
    "        corr, p_value = pearsonr(df['direct_diff'], df[f'{method}_diff'])\n",
    "        print(f\"Direct vs. {method:<20}: r = {corr:.3f} (p = {p_value:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results_plot_exp3a(results_df):\n",
    "\n",
    "    print(\"\\nGenerating results plot for Experiment 3a...\")\n",
    "    \n",
    "    methods = [\"direct\"] + list(PROMPT_TEMPLATES_EXP3a.keys())\n",
    "    accuracies = []\n",
    "    \n",
    "    direct_correct = (results_df['direct_log_prob_grammatical'] > results_df['direct_log_prob_ungrammatical']).sum()\n",
    "    accuracies.append(direct_correct / len(results_df))\n",
    "\n",
    "    for method in PROMPT_TEMPLATES_EXP3a.keys():\n",
    "        tp = (results_df[f'{method}_log_prob_yes_grammatical'] > results_df[f'{method}_log_prob_no_grammatical']).sum()\n",
    "        tn = (results_df[f'{method}_log_prob_no_ungrammatical'] > results_df[f'{method}_log_prob_yes_ungrammatical']).sum()\n",
    "        accuracies.append(((tp / len(results_df)) + (tn / len(results_df))) / 2)\n",
    "\n",
    "    plot_labels = ['Direct', 'MetaQuestionSimple', 'MetaInstruct', 'MetaQuestionComplex']\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    colors = ['#2f2f2f', '#8f6A7B', '#c49a9a', '#f3e1d3']\n",
    "    bars = ax.bar(plot_labels, accuracies, color=colors, edgecolor='black')\n",
    "    \n",
    "    ax.axhline(y=0.5, color='gray', linestyle='--', linewidth=1.5, label='Random Baseline (50%)')\n",
    "\n",
    "    ax.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax.set_title('Experiment 3a: Sentence Judgment (Syntax)', fontsize=14, pad=20)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.tick_params(axis='x', rotation=15, labelsize=10)\n",
    "    ax.legend()\n",
    "    \n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2.0, yval + 0.02, f'{yval:.2%}', ha='center', va='bottom')\n",
    "\n",
    "    output_filename = 'experiment_3a_results.pdf'\n",
    "    plt.savefig(output_filename, bbox_inches='tight')\n",
    "    print(f\"Plot saved as '{output_filename}'\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_detailed_results_exp3a(results_df, filename=\"detailed_experiment_3a_output.csv\"):\n",
    "    results_df.to_csv(filename, index=False)\n",
    "    print(f\"Detailed results for Exp3a saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_on_results_exp3a(results_df):\n",
    "    print(\"\\n\" + \"=\"*25 + \" Analysis of Experiment 3a Results \" + \"=\"*25)\n",
    "    \n",
    "    print(\"\\n[1] Summary of the Experiment\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"This script replicated Experiment 3a (Sentence Judgment) using the '{MODEL_NAME}' model on {len(results_df)} minimal pairs from BLiMP.\")\n",
    "    print(\"We tested the model's ability to distinguish grammatical from ungrammatical sentences.\")\n",
    "    print(\"  - Direct Method: Compared the full log probability of each sentence in a pair.\")\n",
    "    print(\"  - Metalinguistic Methods: Asked the model if each sentence was 'good' and compared its confidence in answering 'Yes'.\")\n",
    "\n",
    "    print(\"\\n[2] Key Findings\")\n",
    "    print(\"-\" * 30)\n",
    "    direct_accuracy = (results_df['direct_log_prob_grammatical'] > results_df['direct_log_prob_ungrammatical']).sum() / len(results_df)\n",
    "    \n",
    "    print(f\"  - Performance: The Direct method achieved an accuracy of {direct_accuracy:.2%}. The metalinguistic prompts performed variably, but generally lower. This again shows that direct access to probabilities is a more sensitive measure of the model's internal knowledge.\")\n",
    "    print(\"  - Consistency: The Pearson correlation between the direct and metalinguistic methods is likely weak. This highlights the core finding of the paper: prompting introduces a significant 'performance' challenge. The model may 'know' a sentence is ungrammatical (assign it low probability) but fail to correctly 'report' this knowledge when asked via a natural language question.\")\n",
    "    print(\"  - Relation to Exp 3b (Preview): The paper finds that metalinguistic performance improves significantly in Experiment 3b, where sentences are presented together for comparison. The relatively low performance here in 3a sets the stage for that important finding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##           MAIN EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading model: deepseek-ai/deepseek-coder-1.3b-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at deepseek-ai/deepseek-coder-1.3b-base and are newly initialized: ['model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Loading dataset for Experiment 3a from: /Users/liangxinyu/Desktop/XinyuLiang-prompting/datasets/exp3/blimp/corpus.csv\n",
      "Loaded and renamed 390 total items.\n",
      "Subsampling to 390 items...\n",
      "\n",
      "Running Experiment 3a on 390 items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 133/390 [31:15<1:00:24, 14.10s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m load_model_and_tokenizer(MODEL_NAME)\n\u001b[1;32m      4\u001b[0m dataset_exp3a \u001b[38;5;241m=\u001b[39m load_and_prepare_dataset_exp3a(DATASET_EXP3a_PATH, NUM_SAMPLES, RANDOM_SEED)\n\u001b[0;32m----> 6\u001b[0m results_df_exp3a \u001b[38;5;241m=\u001b[39m run_experiment_3a(model, tokenizer, dataset_exp3a)\n\u001b[1;32m      8\u001b[0m save_detailed_results_exp3a(results_df_exp3a)\n\u001b[1;32m      9\u001b[0m analyze_results_exp3a(results_df_exp3a)\n",
      "Cell \u001b[0;32mIn[18], line 27\u001b[0m, in \u001b[0;36mrun_experiment_3a\u001b[0;34m(model, tokenizer, dataset)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# UNGRAMMATICAL\u001b[39;00m\n\u001b[1;32m     26\u001b[0m prompt_ungrammatical \u001b[38;5;241m=\u001b[39m template\u001b[38;5;241m.\u001b[39mformat(sentence\u001b[38;5;241m=\u001b[39mungrammatical_sent)\n\u001b[0;32m---> 27\u001b[0m log_probs_ungrammatical \u001b[38;5;241m=\u001b[39m get_log_probs_of_options(model, tokenizer, prompt_ungrammatical, options)\n\u001b[1;32m     28\u001b[0m item_results[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_log_prob_yes_ungrammatical\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m log_probs_ungrammatical[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     29\u001b[0m item_results[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_log_prob_no_ungrammatical\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m log_probs_ungrammatical[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m, in \u001b[0;36mget_log_probs_of_options\u001b[0;34m(model, tokenizer, text_prompt, options)\u001b[0m\n\u001b[1;32m     12\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     13\u001b[0m inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m logits_next \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     15\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlog_softmax(logits_next, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m prompt_ids \u001b[38;5;241m=\u001b[39m tokenizer(prompt, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39minput_ids\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    685\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    689\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    690\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    691\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    692\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    693\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    694\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    695\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    696\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    697\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    698\u001b[0m )\n\u001b[1;32m    700\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    701\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:578\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    570\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    571\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    572\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    576\u001b[0m     )\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    579\u001b[0m         hidden_states,\n\u001b[1;32m    580\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    581\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    582\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    583\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    584\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    585\u001b[0m     )\n\u001b[1;32m    587\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:292\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    289\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    293\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    294\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    295\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    296\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    297\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    298\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    299\u001b[0m )\n\u001b[1;32m    300\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:196\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    194\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    195\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 196\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    198\u001b[0m kv_seq_len \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        model, tokenizer = load_model_and_tokenizer(MODEL_NAME)\n",
    "        dataset_exp3a = load_and_prepare_dataset_exp3a(DATASET_EXP3a_PATH, NUM_SAMPLES, RANDOM_SEED)\n",
    "        \n",
    "        results_df_exp3a = run_experiment_3a(model, tokenizer, dataset_exp3a)\n",
    "        \n",
    "        save_detailed_results_exp3a(results_df_exp3a)\n",
    "        analyze_results_exp3a(results_df_exp3a)\n",
    "        generate_results_plot_exp3a(results_df_exp3a)\n",
    "        comment_on_results_exp3a(results_df_exp3a)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during the execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"\\nChecking code one more time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================= Experiment 3a — Novel Pair Tests =========================\n",
      "\n",
      "==================== Experiment 3a — Single Pair ====================\n",
      "Grammatical   : The keys to the cabinet are on the table.\n",
      "Ungrammatical : The keys to the cabinet is on the table.\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LlamaForCausalLM.forward() got an unexpected keyword argument 'token_type_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m new_pairs_exp3a:\n\u001b[0;32m---> 74\u001b[0m         test_single_item_exp3a(\n\u001b[1;32m     75\u001b[0m             model,\n\u001b[1;32m     76\u001b[0m             tokenizer,\n\u001b[1;32m     77\u001b[0m             grammatical\u001b[38;5;241m=\u001b[39mpair[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrammatical\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     78\u001b[0m             ungrammatical\u001b[38;5;241m=\u001b[39mpair[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mungrammatical\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     79\u001b[0m         )\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel and tokenizer not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 19\u001b[0m, in \u001b[0;36mtest_single_item_exp3a\u001b[0;34m(model, tokenizer, grammatical, ungrammatical)\u001b[0m\n\u001b[1;32m     16\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Direct\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m lp_g \u001b[38;5;241m=\u001b[39m get_full_sentence_log_prob(model, tokenizer, grammatical)\n\u001b[1;32m     20\u001b[0m lp_u \u001b[38;5;241m=\u001b[39m get_full_sentence_log_prob(model, tokenizer, ungrammatical)\n\u001b[1;32m     21\u001b[0m direct_diff \u001b[38;5;241m=\u001b[39m lp_g \u001b[38;5;241m-\u001b[39m lp_u\n",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m, in \u001b[0;36mget_full_sentence_log_prob\u001b[0;34m(model, tokenizer, sentence)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, labels\u001b[38;5;241m=\u001b[39minput_ids)\n\u001b[1;32m     13\u001b[0m log_likelihood \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39moutputs\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m (input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log_likelihood\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: LlamaForCausalLM.forward() got an unexpected keyword argument 'token_type_ids'"
     ]
    }
   ],
   "source": [
    "def test_single_item_exp3a(model, tokenizer, grammatical, ungrammatical):\n",
    "    \"\"\"\n",
    "    Experiment 3a diagnostic on a *single* BLiMP-style minimal pair.\n",
    "    - Direct: compare full-sentence log-likelihoods (grammatical vs. ungrammatical).\n",
    "    - Meta: for each metalinguistic prompt, ask 'Is this a good sentence?'\n",
    "      and compute a per-item Balanced Accuracy (BA):\n",
    "         BA = 0.5 * [ I(Yes>No | grammatical) + I(No>Yes | ungrammatical) ].\n",
    "      As a tie-breaker, report the average margin:\n",
    "         margin = 0.5 * [ (logP_Yes - logP_No)_gram + (logP_No - logP_Yes)_ungram ].\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*20 + \" Experiment 3a — Single Pair \" + \"=\"*20)\n",
    "    print(f\"Grammatical   : {grammatical}\")\n",
    "    print(f\"Ungrammatical : {ungrammatical}\")\n",
    "    print(\"-\" * 62)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Direct\n",
    "    lp_g = get_full_sentence_log_prob(model, tokenizer, grammatical)\n",
    "    lp_u = get_full_sentence_log_prob(model, tokenizer, ungrammatical)\n",
    "    direct_diff = lp_g - lp_u\n",
    "    direct_acc = 1.0 if direct_diff > 0 else 0.0\n",
    "    print(f\"{'Direct:':<18} {'✅' if direct_acc == 1.0 else '❌'}  \"\n",
    "          f\"acc={direct_acc:.2f}  Δ(logP_g - logP_u)={direct_diff:.2f}\")\n",
    "    results.append((\"Direct\", direct_acc, abs(direct_diff)))\n",
    "\n",
    "    # Metalinguistic prompts\n",
    "    options = [\"Yes\", \"No\"]\n",
    "    for method_name, template in PROMPT_TEMPLATES_EXP3a.items():\n",
    "        prompt_g = template.format(sentence=grammatical)\n",
    "        lp_yes_g, lp_no_g = get_log_probs_of_options(model, tokenizer, prompt_g, options)\n",
    "\n",
    "        prompt_u = template.format(sentence=ungrammatical)\n",
    "        lp_yes_u, lp_no_u = get_log_probs_of_options(model, tokenizer, prompt_u, options)\n",
    "\n",
    "        pred_g = (lp_yes_g > lp_no_g)   # Yes\n",
    "        pred_u = (lp_no_u > lp_yes_u)   # No\n",
    "\n",
    "        ba = 0.5 * (float(pred_g) + float(pred_u))\n",
    "        margin = 0.5 * ((lp_yes_g - lp_no_g) + (lp_no_u - lp_yes_u))\n",
    "\n",
    "        print(f\"{method_name+':':<18} \"\n",
    "              f\"{'✅' if ba == 1.0 else ('➖' if ba == 0.5 else '❌')}  \"\n",
    "              f\"BA={ba:.2f}  \"\n",
    "              f\"marg={margin:.2f}  \"\n",
    "              f\"[G: Yes-No={lp_yes_g - lp_no_g:.2f} | U: No-Yes={lp_no_u - lp_yes_u:.2f}]\")\n",
    "\n",
    "        results.append((method_name, ba, margin))\n",
    "\n",
    "    best_name, best_score, best_tie = max(results, key=lambda t: (t[1], t[2]))\n",
    "    print(\"-\" * 62)\n",
    "    label = \"Balanced Acc.\" if best_name != \"Direct\" else \"Accuracy\"\n",
    "    print(f\"Best method: {best_name}   ({label}={best_score:.2f}, tie-break margin={best_tie:.2f})\")\n",
    "\n",
    "\n",
    "new_pairs_exp3a = [\n",
    "    {\n",
    "        \"grammatical\":   \"The keys to the cabinet are on the table.\",\n",
    "        \"ungrammatical\": \"The keys to the cabinet is on the table.\"\n",
    "    },\n",
    "    {\n",
    "        \"grammatical\":   \"Who did Mary say that John met yesterday?\",\n",
    "        \"ungrammatical\": \"Who did Mary say that John met him yesterday?\"\n",
    "    },\n",
    "    {\n",
    "        \"grammatical\":   \"There seem to be many solutions.\",\n",
    "        \"ungrammatical\": \"There seems to be many solutions.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*25 + \" Experiment 3a — Novel Pair Tests \" + \"=\"*25)\n",
    "if 'model' in locals() and 'tokenizer' in locals():\n",
    "    for pair in new_pairs_exp3a:\n",
    "        test_single_item_exp3a(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            grammatical=pair[\"grammatical\"],\n",
    "            ungrammatical=pair[\"ungrammatical\"]\n",
    "        )\n",
    "else:\n",
    "    print(\"Model and tokenizer not found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
